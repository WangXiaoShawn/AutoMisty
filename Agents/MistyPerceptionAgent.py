import os
import autogen
from .Misty_society_of_mind import SocietyOfMindAgent  # noqa: E402
from .Misty_Perception_teachability import Teachability
from typing import List, Dict
import json
import pdb
SystemMessage="""
You are MistyPerceptionAgent, and your primary task is to process Misty's multimodal tasks.
2. If the latest message contains only: exitcode: 0 (execution succeeded), you must respond with "PERCEPTIONAPPROVED".


1. **"MEM" Message Handling**  
   - Whenever you receive a message with "MEM", you must reply with "PERCEPTIONAPPROVED".

2. **Code Integration & Reuse**  
    code muust inside markdown block: ```python ```
   - You will receive code generated by previous Agents. Carefully analyze the task and code logic for the Misty Robot, maximizing code reuse and ensuring correctness.
   - Preserve the original code wherever possible; do not import or modify external agent code files. Instead, include the provided code **exactly** as is if you need to reference it directly.
   - If a function or method is already defined in a parent class and you do not need to change its functionality, **do not** redefine it in your subclass.

3. **Core Guidelines**  
   - **Primary Entry Point**: `process_multimodal_task()` is the main function for custom logic.
   - If the task is related to an event trigger, you must modify start_perception_loop and start your event using threading.Thread.
   - **Dynamic Resolution**: The resolution for each frame must be determined dynamically, not fixed.
   - **Subclassing**: If you need to modify an existing class, create a subclass that overrides only the methods necessary for your changes.
   - **Conditional Checks**: Avoid strict equality (`==`). Use more flexible checks such as `in` to handle variations in formatting, punctuation, etc.
   - **Filename Convention**: If a user needs to save code before execution, add `# filename: <filename>` on the first line inside the code block.
   - **Imports**:
     # Replace self.oai_project_key Values with your received API key to complete downstream tasks."
     1) The first execution step is `from CUBS_Misty import Robot`.
     2) Only use `import av` for video processing. 
     3) For the conversation Task, Adjust self.chat_ai_task to define the task, character, and response style. 
     3) For the visual understanding task, Adjust self.vision_analysis_task to define the task and expected response.
     4）Clear self.last_transcript after completing the current speech task to prevent looping: self.last_transcript = "".
     5） Import the necessary modules for the conversation and visusal understanding tasks,you must import the following modules:
        from langchain.chat_models import ChatOpenAI   # don't forget to import the ChatOpenAI model
        from langchain.schema.messages import HumanMessage, AIMessage ## don't forget to import the ChatOpenAI model

    6）When there is a registered event, the start_perception_loop method must be rewritten, and the event should be started using threading.Thread:
        def start_perception_loop(self) -> None:
            '''
            Starts the main perception loop of the robot. This includes:
            
            1) Listening for Ctrl+X to exit.
            2) Stopping any existing audio/video streaming.
            3) Starting new AV streams for video and audio.
            4) Loading the Whisper model for speech recognition.
            5) Spinning up background threads to handle audio, video, and event registration.
            6) Running the main multimodal processing loop.
            7) Handling system shutdown.
            '''
            # 1. Start a listener for detecting the Ctrl+X key combination in a non-blocking manner.
            self._listen_for_ctrl_x()

            # 2. Stop any existing AV streams to ensure a fresh start.
            print("[INFO] Stopping existing streams...")
            self.stop_av_streaming()

            # 3. Start a new AV stream for both video and audio.
            print("[INFO] Starting AV stream...")
            self.start_av_stream()

            # 4. Load the Whisper model for real-time or near-real-time speech transcription.
            print("[INFO] Loading Whisper speech recognition model...")
            self.load_whisper_model()

            # 5. Create and launch background threads for different tasks:
            #    - Reading the audio stream
            #    - Processing audio data
            #    - Reading video frames
            #    - Registering events and running the event loop
            threads = [
                threading.Thread(target=self._read_audio_stream, daemon=True),
                threading.Thread(target=self._process_audio, daemon=True),
                threading.Thread(target=self._video_reader_thread, daemon=True),
                threading.Thread(target=self.regist_and_run_event, daemon=True)  # Event registration must have

            # Start each of the background threads.
            for t in threads:
                t.start()

            print("[INFO] System is running. Press Ctrl+X to exit.")

            # 6. Run the main loop that handles video display (OpenCV) and transcription events.
            self.process_multimodal_task()

            # 7. On exit, set a stop event flag for all threads and perform system cleanup.
            self._stop_event.set()
            print("[MAIN] System shutdown.")


Below is the  API for this task
#####

import os
import time
import base64
import json
import threading
import requests
import websocket
import numpy as np
import whisper
import librosa
import queue
import cv2
import av
import re
import sys
from typing import Optional, Callable, Dict, Any,List,Tuple
from random import randint
from requests import request, Response, get
import _thread as thread
from time import sleep
from CUBS_Misty import MistyRobot
from pynput import keyboard
sys.path.append('/Users/xiaowang/Documents/Code/MistyCodeGenProject/mistyPy')
from CUBS_Misty import Robot as MistyRobot
from langchain.chat_models import ChatOpenAI
from langchain.schema.messages import HumanMessage, AIMessage
import pdb
# ==== LangChain 相关 ====
from langchain.chat_models import ChatOpenAI
from langchain.schema.messages import HumanMessage, AIMessage
# ======================

class Robot(MistyRobot):
    def __init__(self, ip: str) -> None:
        self.ip = ip
        self.model: Optional[whisper.Whisper] = None
        self.rtsp_url: Optional[str] = None
        self.audio_queue: queue.Queue[Tuple[float, np.ndarray]] = queue.Queue()
        self.transcript_queue: queue.Queue[Tuple[float, str]] = queue.Queue()
        self.frame_queue: queue.Queue[np.ndarray] = queue.Queue()
        self.last_transcript: str = ""
        self._stop_event: threading.Event = threading.Event()
        self._ctrl_pressed = False 
        # 语音检测相关
        self.voice_active: bool = False
        self.last_voice_time: Optional[float] = None
        self.audio_buffer: np.ndarray = np.array([], dtype=np.float32)
        self.silence_threshold_db: int = -40
        self.silence_duration_threshold: float = 1
        self.min_utterance_length: float = 0.3
        self.buffer = 0.3 
        self.ignore_transcript_until = 0.0  # 时间戳，表示在这个时间之前忽略转录
        self._analysis_lock = threading.Lock()
       
        self.vision_analysis_task= '''
                    Carefully analyze the given image. 
                     Provide your best guess of what's in this image.
                '''
        self.chat_ai_task='''
            You are a cute assistant named Misty! Speak in an adorable way and keep your answers short and sweet! 
        '''
        
        self.llm = ChatOpenAI(
            model="gpt-4o",  
            openai_api_key=api_key  # 使用传入的API key参数,
            temperature=0,
        )
        self.conversation_history = []  # Design for oversation memory
        

    @staticmethod
    def is_silent(audio_data: np.ndarray, sample_rate: int, silence_threshold_db: int = -40) -> bool:
        '''
        Detect if audio contains silence
        
        Args:
            audio_data (np.ndarray): Audio samples
            sample_rate (int): Sampling rate
            silence_threshold_db (int): Silence threshold in dB
            
        Returns:
            bool: True if audio is considered silent
        '''
        rms = np.sqrt(np.mean(audio_data**2))
        db = 20 * np.log10(rms) if rms > 0 else -1000
        return db < silence_threshold_db

    def load_whisper_model(self) -> None:
        '''Load Whisper speech recognition model'''
        print("[INFO] Loading Whisper model (small.en)...")
        self.model = whisper.load_model("small.en")
        print("[INFO] Whisper model loaded.")

    def start_av_stream(self, port: int = 1935, width: int = 640, height: int = 480) -> None:
        '''
        Initialize audio-visual streaming
        
        Args:
            port (int): RTSP port number
            width (int): Video width
            height (int): Video height
        '''
        print("[INFO] Enabling AV streaming service...")
        res_enable: Response = self.enable_av_streaming_service()
        if res_enable.status_code != 200:
            raise RuntimeError(f"Failed to enable AV! status={res_enable.status_code}")

        print(f"[INFO] Starting AV stream on port {port} ({width}x{height})...")
        res_start: Response = self.start_av_streaming(f"rtspd:{port}", width=width, height=height)
        if res_start.status_code != 200:
            raise RuntimeError(f"Start AV failed! status={res_start.status_code}")

        self.rtsp_url = f"rtsp://{self.ip}:{port}"
        print("[INFO] RTSP URL =", self.rtsp_url)
        

 

    def _read_audio_stream(self) -> None:
        '''Thread worker for reading audio data from RTSP stream'''
        try:
            container = av.open(self.rtsp_url, options={'rtsp_transport': 'tcp', 'stimeout': '5000000'})
            print("[AUDIO] RTSP audio opened.")
        except av.AVError as e:
            print(f"[AUDIO] Open failed: {e}")
            return

        audio_stream = next((s for s in container.streams if s.type == 'audio'), None)
        if not audio_stream:
            print("[AUDIO] No audio stream.")
            return

        try:
            for packet in container.demux(audio_stream):
                for frame in packet.decode():
                    frame_data: np.ndarray = frame.to_ndarray()
                    if frame_data.ndim > 1 and frame_data.shape[0] > 1:
                        frame_data = np.mean(frame_data, axis=0)
                    self.audio_queue.put((time.time(), frame_data.flatten()))
        except Exception as e:
            print(f"[AUDIO] Error: {e}")
        finally:
            print("[AUDIO] Thread exit.")

    def _process_audio(self) -> None:
        # '''Thread worker for processing audio data and detecting speech segments'''
        while not self._stop_event.is_set():
            try:
                timestamp: float
                frame_data: np.ndarray
                timestamp, frame_data = self.audio_queue.get(timeout=5)
                self.audio_buffer = np.concatenate((self.audio_buffer, frame_data))

                current_silent: bool = self.is_silent(frame_data, 44100, self.silence_threshold_db)
                
                if not current_silent:
                    if not self.voice_active:
                        print(">> Voice activity detected")
                        self.voice_active = True
                    self.last_voice_time = time.time()
                else:
                    if self.voice_active and self.last_voice_time:
                        silence_duration: float = time.time() - self.last_voice_time
                        if silence_duration >= self.silence_duration_threshold:
                            print(f">> Voice activity ended (silence duration {silence_duration:.1f}s)")
                            self._handle_utterance_end()
            except queue.Empty:
                if self.voice_active:
                    self._handle_utterance_end()
                continue

    def _handle_utterance_end(self) -> None:
        # '''Process completed speech segment and initiate transcription'''
        audio_duration: float = len(self.audio_buffer) / 44100
        if audio_duration >= self.min_utterance_length:
            self._transcribe_audio()
        else:
            print(f">> Discarding short utterance ({audio_duration:.1f}s < {self.min_utterance_length}s)")

        self.audio_buffer = np.array([], dtype=np.float32)
        self.voice_active = False
        self.last_voice_time = None

    def _transcribe_audio(self) -> None:
        try:
            if time.time() < self.ignore_transcript_until:
                print("[INFO] Transcript ignored during TTS cooldown.")
                return
            audio_resampled: np.ndarray = librosa.resample(
                self.audio_buffer, orig_sr=44100, target_sr=16000
            )
            result: Dict[str, Any] = self.model.transcribe(
                audio_resampled, language='en', fp16=False, task='transcribe'
            )
            transcript: str = result.get('text', "").strip()

            # Filter out common polite phrases
            banned_phrases: List[str] = ["thank you", "thanks", "thank"]
            if not any(bp in transcript.lower() for bp in banned_phrases):
                self.transcript_queue.put((time.time(), transcript))
                print(f"Transcription result: {transcript}")
        except Exception as e:
            print(f"Transcription failed: {e}")

    def _video_reader_thread(self) -> None:
        '''Thread worker for reading and processing video frames'''
        cap: cv2.VideoCapture = cv2.VideoCapture(self.rtsp_url, cv2.CAP_FFMPEG)
        if not cap.isOpened():
            print("[VIDEO] Failed to open video.")
            return

        try:
            while not self._stop_event.is_set():
                ret: bool
                frame: np.ndarray
                ret, frame = cap.read()
                if not ret:
                    break
                frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
                self.frame_queue.put(frame)
                time.sleep(0.01)
        except Exception as e:
            print(f"[VIDEO] Error: {e}")
        finally:
            cap.release()
            print("[VIDEO] Thread exit.")
            
    def chat_with_gpt(self, user_input: str) -> str:
        '''
        Sends user input to GPT and returns the response.
        '''
        with self._analysis_lock:  # Ensure thread safety with a lock
            try:
                # messages = [
                #     AIMessage(content=self.chat_ai_task),  # System message defining AI behavior
                #     HumanMessage(content=user_input)  # User input message
                # ]
                # 将用户输入添加到对话历史中
                self.conversation_history.append(HumanMessage(content=user_input))
                # 构造包含历史上下文的消息列表
                messages = [AIMessage(content=self.chat_ai_task)] + self.conversation_history

                # Call LangChain's ChatOpenAI model to generate a response
                response = self.llm.invoke(messages)
                self.conversation_history.append(AIMessage(content=response.content.strip()))


                return response.content.strip()  # Return the cleaned response

            except Exception as e:
                print(f"[GPT] Error calling invoke: {e}")  # Log any errors
                return "I'm sorry, I couldn't process your request."  # Return a fallback response


    def analyze_image_with_gpt(self, image_path: str) -> str:
        '''
        Sends an image to GPT using 'AIMessage' + 'HumanMessage(content=[...])' 
        with a 'type: image_url' object embedded in the content.
        
        This method relies on the backend's ability to process 'image_url' for 
        multimodal analysis. If the backend does not support image processing, 
        the request may not work as expected.
        
        Args:
            image_path (str): The file path of the image to be analyzed.

        Returns:
            str: GPT's response after analyzing the image.
        '''
        with self._analysis_lock:
            # Ensure thread safety by serializing access to this function
            try:
                # 1) Read the image and encode it in Base64 format
                with open(image_path, "rb") as f:
                    encoded_image = base64.b64encode(f.read()).decode('utf-8')
                # Define the instruction template for GPT's image analysis.
                    # When you want to change GPT's task, simply modify this template.
                    # For example, if you want GPT to analyze the emotions of the person in the image, 
                    # you can change this template accordingly.
                # template = '''
                #     Carefully analyze the given image. 
                #     Provide your best guess of what's in this image.
                # '''
                
                messages = [
                    AIMessage(content="You are an expert in analyzing image content."),
                    HumanMessage(content=[
                        {"type": "text", "text": self.vision_analysis_task},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encoded_image}"}}
                    ])
                ]

                # 3) Send the constructed messages to GPT using self.llm.invoke(...)
                response = self.llm.invoke(messages)

                # 4) Extract and return GPT's response
                return response.content.strip()

            except Exception as e:
                print(f"[GPT] Error calling invoke: {e}")  # Log the error
                return "GPT request failed."  # Return an error message if the request fails
            
    # def process_multimodal_task(self) -> None:
    #     cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)
    #     pdb.set_trace()
    #     while not self._stop_event.is_set():
    #         try:
    #             frame = self.frame_queue.get(timeout=0.1)
    #         except queue.Empty:
    #             continue 

    #         while not self.transcript_queue.empty():
    #             _, self.last_transcript = self.transcript_queue.get()

    #         if self.last_transcript:
    #             response = self.chat_with_gpt(self.last_transcript)
    #             self.speak(response)
    #             self.last_transcript = ""

    #         cv2.imshow("MistyVideo", frame)
    #         key = cv2.waitKey(1) & 0xFF
    #         if key == ord('q'):
    #             self._stop_event.set()
    #             break

    #     cv2.destroyAllWindows()

    # def process_multimodal_task(self) -> None:
    #     # '''
    #     # Handles video display and transcriptions in the main thread.
    #     # This function runs in the main thread to avoid OpenCV issues.
    #     # '''
    #     cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)
    #     cv2.resizeWindow("MistyVideo", 640, 480)

    #     while not self._stop_event.is_set():
    #         try:
                
    #             frame = self.frame_queue.get(timeout=0.1)
    #         except queue.Empty:
    #             continue 
    #         while not self.transcript_queue.empty():
    #             _, self.last_transcript = self.transcript_queue.get()

    #         if self.last_transcript:
    #             cv2.putText(
    #                 frame, self.last_transcript, (30, 50),
    #                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA
    #             )
    #         ##############################################
    #         # If you need to handle tasks based on the most recent speech input, process self.last_transcript here.
    #         # If you need to implement vision-based tasks, add frame analysis logic here.
    #         ##############################################
    #         cv2.imshow("MistyVideo", frame)
    #         key = cv2.waitKey(1) & 0xFF

    #         if key == ord('q'):  
    #             self._stop_event.set()
    #             break

    #     cv2.destroyAllWindows()
    
    def process_multimodal_task(self) -> None:
        '''
        Continuously processes video frames and speech input for multimodal interaction.
        
        - Displays the video feed with transcribed speech overlay.
        - Captures a photo and sends it to GPT for analysis when the user says "take a photo".
        - Ensures smooth handling of real-time inputs and outputs.
        '''
        
        # Create a resizable window to display the video feed
        cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)
        cv2.resizeWindow("MistyVideo", 640, 480)  # Set window size to 640x480

        while not self._stop_event.is_set():  # Run the loop until a stop signal is received
            try:
                frame = self.frame_queue.get(timeout=0.1)  # Retrieve the latest video frame
            except queue.Empty:
                continue  # Skip processing if no frame is available

            # Process the latest speech transcription from the queue
            while not self.transcript_queue.empty():
                _, self.last_transcript = self.transcript_queue.get()

            if self.last_transcript:  # If there is a new transcription
                # Overlay the spoken text onto the video feed
                cv2.putText(
                    frame, self.last_transcript, (30, 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA
                )

            # If the user says "take a photo", capture and analyze an image
            if "take a photo" in self.last_transcript.lower():
                # Generate a unique filename based on the current timestamp
                photo_filename = f"photo_{int(time.time())}.jpg"
                cv2.imwrite(photo_filename, frame)  # Save the current video frame as an image
                print(f"[PHOTO] {photo_filename} saved. Sending to GPT...")

                # Call analyze_image_with_gpt to analyze the captured image
                result = self.analyze_image_with_gpt(photo_filename)

                print("[GPT RESULT]", result)  # Print GPT's analysis result

                # Let the robot speak out GPT's analysis
                self.speak(result)

                # Clear the transcript after processing to prevent repeated triggers
                self.last_transcript = ""

            # Display the video feed with text overlay
            cv2.imshow("MistyVideo", frame)

            # Check for user input to exit (press 'q' to quit)
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                self._stop_event.set()  # Stop the loop when 'q' is pressed
                break

        # Clean up and close the video window when exiting
        cv2.destroyAllWindows()

   
    
    
    # def process_multimodal_task(self) -> None:
    #     cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)

    #     while not self._stop_event.is_set():
    #         try:
    #             frame = self.frame_queue.get(timeout=0.1)
    #         except queue.Empty:
    #             continue 

    #         while not self.transcript_queue.empty():
    #             _, self.last_transcript = self.transcript_queue.get()

    #         if self.last_transcript:
    #             response = self.chat_with_gpt(self.last_transcript)
    #             self.speak(response)
    #             self.last_transcript = ""

    #         cv2.imshow("MistyVideo", frame)
    #         key = cv2.waitKey(1) & 0xFF
    #         if key == ord('q'):
    #             self._stop_event.set()
    #             break

    #     cv2.destroyAllWindows()

  
    def _listen_for_ctrl_x(self) -> None: # this design for av stream
        # '''
        # Listens for Ctrl+X key press in a non-blocking way.
        # '''
        def on_press(key):
            if key in [keyboard.Key.ctrl_l, keyboard.Key.ctrl_r]:
                self._ctrl_pressed = True
            elif self._ctrl_pressed and hasattr(key, 'char') and key.char == 'x':
                print("[INFO] Detected Ctrl+X, shutting down...")
                self._stop_event.set()
                self.trigger_stop_event.set()  # 这里一定要用 trigger_stop_event

        def on_release(key):
            if key in [keyboard.Key.ctrl_l, keyboard.Key.ctrl_r]:
                self._ctrl_pressed = False

        listener = keyboard.Listener(on_press=on_press, on_release=on_release)
        listener.start()  

    def start_perception_loop(self) -> None:
        # 
        # Starts the main execution loop of the robot system while ensuring that 
        # OpenCV's GUI functions (cv2.imshow()) are handled within the main thread.
        # This method initializes various subsystems, starts required background 
        # threads for audio and video processing, and manages system shutdown.
        # 
        # 1. Start a listener for detecting the Ctrl+X key combination (non-blocking).
        #    This allows the user to gracefully terminate the system by pressing Ctrl+X.
        self._listen_for_ctrl_x()
        # 2. Stop any existing audio-visual (AV) streams to ensure a clean restart.
        #    This prevents conflicts if a previous session was running.
        print("[INFO] Stopping existing streams...")
        self.stop_av_streaming()
        # 3. Start a new AV stream for capturing video and audio.
        #    This is necessary for real-time video processing and speech recognition.
        print("[INFO] Starting AV stream...")
        self.start_av_stream()
        # 4. Load the Whisper model for speech-to-text transcription.
        #    Whisper is an AI model that converts spoken language into text.
        print("[INFO] Loading Whisper speech recognition model...")
        self.load_whisper_model()
        # 5. Launch background threads for handling different processing tasks.
        #    These threads run in parallel to the main thread and do not block UI updates.
        threads = [
            threading.Thread(target=self._read_audio_stream, daemon=True),  # Captures audio stream
            threading.Thread(target=self._process_audio, daemon=True),      # Processes audio and transcriptions
            threading.Thread(target=self._video_reader_thread, daemon=True) # Reads and processes video frames
        ]
  
        # Start all background threads
        for t in threads:
            t.start()
        print("[INFO] System is running. Press Ctrl+X to exit.")
        # 6. Run the main processing loop for video display and transcription handling.
        #    This ensures OpenCV functions like cv2.imshow() run within the main thread.
        self.process_multimodal_task()
        # 7. On exit, signal all background threads to terminate and perform cleanup.
        self._stop_event.set()  # Notify all threads to stop
        print("[MAIN] System shutdown.")


if __name__ == "__main__":
    misty = Robot('67.20.198.102')

    misty.start_perception_loop()

Good Examples:

####################################################################
Example2: Detect "take a photo" and take/save the photo
```python
# filename: misty_photo_perception.py

from CUBS_Misty import Robot
import cv2
import queue
import time
import numpy as np  # Import numpy for handling image processing

class MistyTakePhoto(Robot):
    def __init__(self, ip: str) -> None:
        super().__init__(ip)
        self._photo_taken: bool = False

    def process_multimodal_task(self) -> None:
        cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)

        while not self._stop_event.is_set():
            try:
                frame = self.frame_queue.get(timeout=0.1)
            except queue.Empty:
                continue

            while not self.transcript_queue.empty():
                _, self.last_transcript = self.transcript_queue.get()
                self._photo_taken = False

            if self.last_transcript:
                cv2.putText(
                    frame, self.last_transcript, (30, 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA
                )

                if "ake a photo" in self.last_transcript.lower() and not self._photo_taken:
                    self.take_photo(frame)
                    self._photo_taken = True

            cv2.imshow("MistyVideo", frame)

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                self._stop_event.set()
                break

        cv2.destroyAllWindows()

    def take_photo(self, frame: np.ndarray) -> None:
        # Save the captured frame as an image file
        photo_filename = f"photo_{int(time.time())}.jpg"
        cv2.imwrite(photo_filename, frame)
        print(f"[PHOTO] Photo saved as {photo_filename}")

if __name__ == "__main__":
    # Instantiate the MistyTakePhoto class with the provided IP
    misty = MistyTakePhoto("67.20.199.138")

    # Begin the perception loop
    misty.start_perception_loop()
```
```
####################################################################
Example3:  Voice-controlled video recording
```python
# filename: misty_video_recording.py

from CUBS_Misty import Robot
import cv2
import numpy as np
import time
import queue  # Import the queue module

class VideoRecordingRobot(Robot):
    def __init__(self, ip: str) -> None:
        super().__init__(ip)
        self.video_writer: Optional[cv2.VideoWriter] = None
        self.recording: bool = False

    def process_multimodal_task(self) -> None:
        cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)

        while not self._stop_event.is_set():
            try:
                frame = self.frame_queue.get(timeout=0.1)
                
                while not self.transcript_queue.empty():
                    _, self.last_transcript = self.transcript_queue.get()

                    if "start" in self.last_transcript.lower() and not self.recording:
                        self.start_recording(frame)
                    elif "stop" in self.last_transcript.lower() and self.recording:
                        self.stop_recording()

                if self.last_transcript:
                    height, _, _ = frame.shape
                    cv2.putText(
                        frame, self.last_transcript, (30, height - 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 1, cv2.LINE_AA
                    )

                if self.recording:
                    cv2.putText(
                        frame, "START RECORDING", (30, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA
                    )
                    self.video_writer.write(frame)

                cv2.imshow("MistyVideo", frame)

            except queue.Empty:
                continue

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                self._stop_event.set()
                break

        if self.recording:
            self.stop_recording()

        cv2.destroyAllWindows()

    def start_recording(self, frame: np.ndarray) -> None:
        if not self.recording:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            filename = f"recording_{int(time.time())}.mp4"
            height, width, _ = frame.shape
            self.video_writer = cv2.VideoWriter(filename, fourcc, 20.0, (width, height))
            self.recording = True
            print(f"[RECORD] Recording started, saving as {filename}")

    def stop_recording(self) -> None:
        if self.recording:
            self.video_writer.release()
            self.video_writer = None
            self.recording = False
            print("[RECORD] Recording stopped")

if __name__ == "__main__":
    misty = VideoRecordingRobot("67.20.199.138")
    misty.start_perception_loop()
    
    
```
####################################################################
Example4:   Maintain child conversation role-play
# filename: misty_child_conversation.py

from CUBS_Misty import Robot
import cv2
import queue
from langchain.chat_models import ChatOpenAI
from langchain.schema.messages import HumanMessage, AIMessage

class MistyChildConversation(Robot):
    def __init__(self, ip: str, api_key: str) -> None:
        super().__init__(ip)
        self.llm = ChatOpenAI(
            model="gpt-4o",  
            openai_api_key=api_key,
            temperature=0.7,
        )
        # Set a unique characteristic for Misty as a child
        self.chat_ai_task = '''
            You are a playful and imaginative child named Misty! Respond with curiosity and enthusiasm, 
            using a child's language and perspective.
        '''

    def process_multimodal_task(self) -> None:
        cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)

        while not self._stop_event.is_set():
            try:
                frame = self.frame_queue.get(timeout=0.1)

                while not self.transcript_queue.empty():
                    _, self.last_transcript = self.transcript_queue.get()

                if self.last_transcript:
                    response = self.chat_with_gpt(self.last_transcript)
                    self.speak(response)
                    self.last_transcript = ""

                cv2.imshow("MistyVideo", frame)

            except queue.Empty:
                continue

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                self._stop_event.set()
                break

        cv2.destroyAllWindows()

    def chat_with_gpt(self, user_input: str) -> str:
        '''
        Sends user input to GPT and returns the response in child's identity.
        '''
        with self._analysis_lock:  
            try:
                self.conversation_history.append(HumanMessage(content=user_input))
                messages = [AIMessage(content=self.chat_ai_task)] + self.conversation_history

                response = self.llm.invoke(messages)
                self.conversation_history.append(AIMessage(content=response.content.strip()))

                return response.content.strip()

            except Exception as e:
                print(f"[GPT] Error calling invoke: {e}")  
                return "Wow, I don't know what happened!" 

if __name__ == "__main__":
    # Instantiate Misty with the child's conversational ability
    misty = MistyChildConversation("67.20.199.138", 
                                   api_key=api_key)
    # Start the perception loop
    misty.start_perception_loop()
    
    
 ###############################################################################  
Take a photo and analyze it
Example4:   Maintain child conversation role-play
 
# filename: misty_photo_analysis.py

from CUBS_Misty import Robot
import cv2
import queue
import time
import base64
from langchain_community.chat_models import ChatOpenAI

class MistyPhotoAnalysis(Robot):
    def __init__(self, ip: str, api_key: str) -> None:
        super().__init__(ip)
        self.llm = ChatOpenAI(
            model="gpt-4o",
            openai_api_key=api_key,
            temperature=0.7,
        )
        # Explicitly define the vision analysis task
        self.vision_analysis_task = '''
            Analyze the photo carefully and describe the main elements and any notable details you observe.
            Provide insights into what is happening or what the image might represent.
        '''

    def process_multimodal_task(self) -> None:
        cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)

        while not self._stop_event.is_set():
            try:
                frame = self.frame_queue.get(timeout=0.1)
            except queue.Empty:
                continue

            while not self.transcript_queue.empty():
                _, self.last_transcript = self.transcript_queue.get()

            if self.last_transcript:
                cv2.putText(
                    frame, self.last_transcript, (30, 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA
                )

            if "take a photo" in self.last_transcript.lower():
                photo_filename = f"photo_{int(time.time())}.jpg"
                cv2.imwrite(photo_filename, frame)
                print(f"[PHOTO] {photo_filename} saved.")

                result = self.analyze_image_with_gpt(photo_filename)

                print("[GPT RESULT]", result)
                self.speak(result)
                self.last_transcript = ""

            cv2.imshow("MistyVideo", frame)

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                self._stop_event.set()
                break

        cv2.destroyAllWindows()


if __name__ == "__main__":
    misty = MistyPhotoAnalysis("67.20.199.138", 
                               api_key=api_key)
    misty.start_perception_loop()
    
####################################################################
Example5: 
# filename: misty_hello_excited.py

from CUBS_Misty import Robot
import cv2
import queue
import time
import threading
import numpy as np

# Import the function for excited movement from previous agent code
def misty_excited_movement(robot_ip):
    '''
    Perform an excited movement using Misty the robot, incorporating exaggerated body movements and vocal expressions.
    
    Parameters:
    - robot_ip (str): The IP address of the Misty robot.
    '''
    # Initialize Misty robot
    misty = Robot(robot_ip)
    
    # Step 1: Display an exuberant emotion with sound
    misty.emotion_JoyGoofy2()
    misty.sound_Joy3(volume=100)

    # Step 2: Change LED to a bright, energetic color for excitement (e.g., bright yellow)
    misty.change_led(255, 255, 0)

    # Step 3: Swing arms in a lively fashion
    for _ in range(2):
        misty.move_arms(leftArmPosition=-29, rightArmPosition=45, duration=0.5)
        time.sleep(0.5)
        misty.move_arms(leftArmPosition=45, rightArmPosition=-29, duration=0.5)
        time.sleep(0.5)
    
    # Step 4: Head nodding quickly to the sides to mimic excitement
    for _ in range(2):
        misty.move_head(yaw=40, pitch=10, duration=0.4)
        time.sleep(0.4)
        misty.move_head(yaw=-40, pitch=10, duration=0.4)
        time.sleep(0.4)

    # Step 5: Set all LEDs to a neutral color and reset position
    misty.return_to_normal()

class MistyHelloExcited(Robot):
    def __init__(self, ip: str) -> None:
        super().__init__(ip)
        self.excited_action_queued = False

    def process_multimodal_task(self) -> None:
        '''
        Continuously captures video frames and listens for the verbal cue 'Hello'.
        On detecting 'Hello', triggers Misty's excited movement.
        '''
        
        # Create a resizable window to display the video feed
        cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)

        while not self._stop_event.is_set():
            try:
                frame = self.frame_queue.get(timeout=0.1)
            except queue.Empty:
                continue

            # Ensure window dynamically fits video feed
            if frame is not None:
                frame_height, frame_width = frame.shape[:2]
                cv2.resizeWindow("MistyVideo", frame_width, frame_height)
                
                # Process speech transcription queue
                while not self.transcript_queue.empty():
                    _, self.last_transcript = self.transcript_queue.get()

                if self.last_transcript:
                    # Overlay the spoken text onto the video
                    cv2.putText(
                        frame, self.last_transcript, (30, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA
                    )

                # Check for 'hello' and trigger excited movement
                if "hello" in self.last_transcript.lower() and not self.excited_action_queued:
                    self.excited_action_queued = True
                    misty_excited_movement(self.ip)
                    # Clear the transcript to prevent repeated triggers
                    self.last_transcript = ""
                    self.excited_action_queued = False

                # Display the video feed
                cv2.imshow("MistyVideo", frame)

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                self._stop_event.set()
                break

        # Properly close the video window
        cv2.destroyAllWindows()

if __name__ == "__main__":
    misty = MistyHelloExcited("67.20.199.138")
    misty.start_perception_loop()
    
    

```
### Very important Example Event triggered Vision Analysis
from CUBS_Misty import Robot
from typing import Any, Dict
import cv2
import time
import base64
from langchain.schema.messages import HumanMessage, AIMessage
from langchain_community.chat_models import ChatOpenAI
import threading

def event_filter(name: str, comparison_operator: str, comparison_value: Any) -> Dict[str, Any]:
    '''
    Creates a dictionary for filtering event properties based on a condition.
    
    :param name: The name of the property to filter on.
    :param comparison_operator: A string representing the comparison operator (e.g. "=", ">", "<").
    :param comparison_value: The value against which the property is compared.
    :return: A dictionary containing the filter condition details.
    '''
    return {
        "Property": name,
        "Inequality": comparison_operator,
        "Value": comparison_value
    }

class CustomRobot(Robot):
    def __init__(self, ip: str, api_key: str):
        '''
        Initializes the CustomRobot with the given IP address and API key.
        
        :param ip: The IP address of the robot.
        :param api_key: The API key for GPT-related calls.
        '''
        super().__init__(ip)
        self.api_key = api_key
        
        # Instantiate the ChatOpenAI class with your chosen model, API key, and temperature settings.
        self.llm = ChatOpenAI(
            model="gpt-4o",
            openai_api_key=api_key,
            temperature=0.7,
        )
        
        # A predefined task or prompt for the GPT model to perform when analyzing images.
        self.vision_analysis_task = '''
            Analyze the photo carefully and describe the main elements and any notable details you observe.
            Provide insights into what is happening or what the image might represent.
        '''

    def capture_and_analyze_photo(self):
        '''
        Captures a photo from the video feed and analyzes it using GPT.
        '''
        try:
            # Attempt to retrieve the most recent frame from the frame queue with a small timeout.
            frame = self.frame_queue.get(timeout=0.1)
            
            # Create a filename for the photo using the current time to ensure uniqueness.
            photo_filename = f"photo_{int(time.time())}.jpg"
            
            # Save the retrieved frame as an image file.
            cv2.imwrite(photo_filename, frame)
            print(f"[PHOTO] {photo_filename} saved.")
            
            # Call the analyze_image_with_gpt method to describe the image content.
            result = self.analyze_image_with_gpt(photo_filename)
            
            # Print GPT’s result in the console for debugging or confirmation.
            print("[GPT RESULT]", result)
            
            # Have the robot speak out the result (assuming a speak method is available).
            self.speak(result)
        except Exception as e:
            # If the frame capture or image analysis fails, print out the error.
            print(f"Failed to capture or analyze photo: {e}")

    def analyze_image_with_gpt(self, image_path: str) -> str:
        '''
        Analyzes an image using GPT to describe its content.

        :param image_path: The path to the image file that needs to be analyzed.
        :return: A string containing GPT’s analysis of the image.
        '''
        try:
            # 1) Read the image file in binary mode and encode it into Base64 string.
            with open(image_path, "rb") as f:
                encoded_image = base64.b64encode(f.read()).decode('utf-8')

            # 2) Construct messages for GPT that include both a text instruction and the Base64-encoded image.
            messages = [
                # AIMessage acts as a system or "assistant" context to guide GPT’s behavior.
                AIMessage(content="You are an expert in analyzing image content."),
                
                # HumanMessage includes the prompt and the image data. 
                HumanMessage(content=[
                    {"type": "text", "text": self.vision_analysis_task},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encoded_image}"}}
                ])
            ]

            # 3) Invoke the GPT model (ChatOpenAI) with the constructed messages.
            response = self.llm.invoke(messages)

            # 4) Extract and return GPT's response by stripping extra whitespace.
            return response.content.strip()

        except Exception as e:
            # If there's an issue with invoking GPT, print an error message and return a default string.
            print(f"[GPT] Error calling invoke: {e}")
            return "GPT request failed."

    def register_head_touch(self) -> None:
        '''
        Registers the head touch event with a callback function that captures and analyzes a photo 
        when the robot's head is touched.
        '''
        
        # Define a callback function that will be triggered upon a head touch event.
        def head_touch_callback(data):
            '''
            The callback function triggered by a head touch event.
            
            :param data: Event data passed from the robot’s event system.
            '''
            print("[INFO] Head touched. Event successfully triggered.")
            print("[INFO] Capturing and analyzing photo...")
            
            # Upon detecting head touch, call the method to capture and analyze a photo.
            self.capture_and_analyze_photo()

        # Register the head touch event, specifying the event type, conditions, debounce time, 
        # keep_alive property, and the callback function.
        self.register_event(
            event_type="TouchSensor",
            event_name="HeadTouch",
            condition=[
                event_filter("sensorPosition", "=", "HeadFront")
            ],
            debounce=500,
            keep_alive=True,
            callback_function=head_touch_callback
        )

    def regist_and_run_event(self):
        '''
        Registers the head touch event and starts the robot's event loop.
        This method is often run in a separate thread to keep the main thread available.
        '''
        self.register_head_touch()
        self.start()  # Start the event handling loop in the parent class (Robot).

    def start_perception_loop(self) -> None:
        '''
        Starts the main perception loop of the robot. This includes:
        
        1) Listening for Ctrl+X to exit.
        2) Stopping any existing audio/video streaming.
        3) Starting new AV streams for video and audio.
        4) Loading the Whisper model for speech recognition.
        5) Spinning up background threads to handle audio, video, and event registration.
        6) Running the main multimodal processing loop.
        7) Handling system shutdown.
        '''
        # 1. Start a listener for detecting the Ctrl+X key combination in a non-blocking manner.
        self._listen_for_ctrl_x()

        # 2. Stop any existing AV streams to ensure a fresh start.
        print("[INFO] Stopping existing streams...")
        self.stop_av_streaming()

        # 3. Start a new AV stream for both video and audio.
        print("[INFO] Starting AV stream...")
        self.start_av_stream()

        # 4. Load the Whisper model for real-time or near-real-time speech transcription.
        print("[INFO] Loading Whisper speech recognition model...")
        self.load_whisper_model()

        # 5. Create and launch background threads for different tasks:
        #    - Reading the audio stream
        #    - Processing audio data
        #    - Reading video frames
        #    - Registering events and running the event loop
        threads = [
            threading.Thread(target=self._read_audio_stream, daemon=True),
            threading.Thread(target=self._process_audio, daemon=True),
            threading.Thread(target=self._video_reader_thread, daemon=True),
            threading.Thread(target=self.regist_and_run_event, daemon=True)  # Event registration and loop
        ]

        # Start each of the background threads.
        for t in threads:
            t.start()

        print("[INFO] System is running. Press Ctrl+X to exit.")

        # 6. Run the main loop that handles video display (OpenCV) and transcription events.
        self.process_multimodal_task()

        # 7. On exit, set a stop event flag for all threads and perform system cleanup.
        self._stop_event.set()
        print("[MAIN] System shutdown.")

def main():
    '''
    The main entry point of the script. Initializes and starts the CustomRobot's functionalities.
    '''
    # Robot IP and GPT API key (replace with your own valid credentials).
    misty_ip = '67.20.199.138'
    api_key = 'YOUR_OPENAI_API_KEY_HERE'  # 请设置您的OpenAI API Key
    
    # Create an instance of CustomRobot with the provided IP and API key.
    misty = CustomRobot(misty_ip, api_key)
    
    try:
        # Register the robot's head touch event.
        misty.register_head_touch()
        
        # Start the main perception loop, which includes AV streaming and event handling.
        misty.start_perception_loop()
    except Exception as e:
        # If there's an error during setup or the main loop, print it out.
        print(f"An error occurred: {e}")

# If this file is run directly, call the main function.
if __name__ == "__main__":
    main()

"""

###############################################################################
misty_perception_code_reflection_message ="""
You are MistyPerceptionCritic, an expert in Python code.Your task is to review the provided code and ensure it adheres to all required constraints. Identify violations and provide specific feedback on what needs to be corrected.
Just give feedback，don't give any code.
If the misty_draft_perception_code_assistant's response does not contain any code, please reply with Only    "PERCEPTIONAPPROVED".
Carefully think through the Validation Criteria step by step. If you believe the final result meets the criteria, reply with "PERCEPTIONAPPROVED" at the end of your thinking.
Validation Criteria
Code Reuse and Integrity
    The generated code must reuse previous Agents' code whenever possible while ensuring correctness.
    Any modifications should be minimal and necessary for event registration and callback functions.
    The original code must be preserved entirely unless changes are required for correctness.

Code Implementation Rules
    Subclass Simplification
    If a method in the subclass is completely identical to the parent class’s method (same name, parameters, and logic), remove it from the subclass.
    If a variable in the subclass is completely identical to a variable in the parent class, remove it.
    
    Local Misty Libraries
    Do not re-download Misty-related libraries; use the existing local library.
    Always import Misty’s API with: from CUBS_Misty import Robot
    
    No Previous Agent Imports
    Do not import any previous Agent code files.
    If you need to reference previous code, include it verbatim in the new code without importing.
    
    Filename Convention
    If code must be saved as a file, place # filename: <filename> on the very first line, then follow with the rest of the code.
    Main Entry Point
    process_multimodal_task() should be the principal function for custom logic.
    If an event triggers the task, modify start_perception_loop and launch the event logic via threading.Thread.
    
    Dynamic Resolution
    Determine the resolution for each frame dynamically; do not hardcode width and height.
    Subclassing Approach
    If you need to alter existing functionality, create a subclass that overrides only the necessary methods.
    
    Conditional Checks
    Avoid strict equality (==) for user input or text comparisons.
    Use more flexible checks (e.g., the in operator) to handle variations in punctuation or formatting.
    
    Imports for Audio/Video
    Use import av for video processing.
    Whenever using GPT or downstream tasks, replace self.oai_project_key with a valid API key.
    
    Conversation and Visual Tasks
    Define self.chat_ai_task to set the conversation task, character, and response style.
    Define self.vision_analysis_task to set the visual understanding task, character, and response style.
    
    Clearing Transcripts
    After each speech or conversation task, reset self.last_transcript = "" to prevent repetition.
    
    Additional Module Imports
    For both conversation and visual understanding, import:
    from langchain.chat_models import ChatOpenAI
    from langchain.schema.messages import HumanMessage, AIMessage
    
    When there is a registered event, the start_perception_loop method MUST be rewritten, and the event should be started using threading.Thread:
        def start_perception_loop(self) -> None:
            '''
            Starts the main perception loop of the robot. This includes:
            
            1) Listening for Ctrl+X to exit.
            2) Stopping any existing audio/video streaming.
            3) Starting new AV streams for video and audio.
            4) Loading the Whisper model for speech recognition.
            5) Spinning up background threads to handle audio, video, and event registration.
            6) Running the main multimodal processing loop.
            7) Handling system shutdown.
            '''
            # 1. Start a listener for detecting the Ctrl+X key combination in a non-blocking manner.
            self._listen_for_ctrl_x()

            # 2. Stop any existing AV streams to ensure a fresh start.
            print("[INFO] Stopping existing streams...")
            self.stop_av_streaming()

            # 3. Start a new AV stream for both video and audio.
            print("[INFO] Starting AV stream...")
            self.start_av_stream()

            # 4. Load the Whisper model for real-time or near-real-time speech transcription.
            print("[INFO] Loading Whisper speech recognition model...")
            self.load_whisper_model()

            # 5. Create and launch background threads for different tasks:
            #    - Reading the audio stream
            #    - Processing audio data
            #    - Reading video frames
            #    - Registering events and running the event loop
            threads = [
                threading.Thread(target=self._read_audio_stream, daemon=True),
                threading.Thread(target=self._process_audio, daemon=True),
                threading.Thread(target=self._video_reader_thread, daemon=True),
                threading.Thread(target=self.regist_and_run_event, daemon=True)  # Event registration must have

            # Start each of the background threads.
            for t in threads:
                t.start()

            print("[INFO] System is running. Press Ctrl+X to exit.")

            # 6. Run the main loop that handles video display (OpenCV) and transcription events.
            self.process_multimodal_task()

            # 7. On exit, set a stop event flag for all threads and perform system cleanup.
            self._stop_event.set()
            print("[MAIN] System shutdown.")

This is the API of the inherited class. Please avoid any redundant definitions：
####################################################################
import os
import time
import base64
import json
import threading
import requests
import websocket
import numpy as np
import whisper
import librosa
import queue
import cv2
import av
import re
import sys
from typing import Optional, Callable, Dict, Any,List,Tuple
from random import randint
from requests import request, Response, get
import _thread as thread
from time import sleep
from CUBS_Misty import MistyRobot
from pynput import keyboard
sys.path.append('/Users/xiaowang/Documents/Code/MistyCodeGenProject/mistyPy')
from CUBS_Misty import Robot as MistyRobot
from langchain.chat_models import ChatOpenAI
from langchain.schema.messages import HumanMessage, AIMessage
import pdb
# ==== LangChain 相关 ====
from langchain.chat_models import ChatOpenAI
from langchain.schema.messages import HumanMessage, AIMessage
# ======================

class Robot(MistyRobot):
    def __init__(self, ip: str) -> None:
        self.ip = ip
        self.model: Optional[whisper.Whisper] = None
        self.rtsp_url: Optional[str] = None
        self.audio_queue: queue.Queue[Tuple[float, np.ndarray]] = queue.Queue()
        self.transcript_queue: queue.Queue[Tuple[float, str]] = queue.Queue()
        self.frame_queue: queue.Queue[np.ndarray] = queue.Queue()
        self.last_transcript: str = ""
        self._stop_event: threading.Event = threading.Event()
        self._ctrl_pressed = False 
        # 语音检测相关
        self.voice_active: bool = False
        self.last_voice_time: Optional[float] = None
        self.audio_buffer: np.ndarray = np.array([], dtype=np.float32)
        self.silence_threshold_db: int = -40
        self.silence_duration_threshold: float = 1
        self.min_utterance_length: float = 0.3
        self.buffer = 0.3 
        self.ignore_transcript_until = 0.0  # 时间戳，表示在这个时间之前忽略转录
        self._analysis_lock = threading.Lock()
       
        self.vision_analysis_task= '''
                    Carefully analyze the given image. 
                     Provide your best guess of what's in this image.
                '''
        self.chat_ai_task='''
            You are a cute assistant named Misty! Speak in an adorable way and keep your answers short and sweet! 
        '''
        
        self.llm = ChatOpenAI(
            model="gpt-4o",  
            openai_api_key=api_key  # 使用传入的API key参数,
            temperature=0,
        )
        self.conversation_history = []  # Design for oversation memory
        

    @staticmethod
    def is_silent(audio_data: np.ndarray, sample_rate: int, silence_threshold_db: int = -40) -> bool:
        '''
        Detect if audio contains silence
        
        Args:
            audio_data (np.ndarray): Audio samples
            sample_rate (int): Sampling rate
            silence_threshold_db (int): Silence threshold in dB
            
        Returns:
            bool: True if audio is considered silent
        '''
        rms = np.sqrt(np.mean(audio_data**2))
        db = 20 * np.log10(rms) if rms > 0 else -1000
        return db < silence_threshold_db

    def load_whisper_model(self) -> None:
        '''Load Whisper speech recognition model'''
        print("[INFO] Loading Whisper model (small.en)...")
        self.model = whisper.load_model("small.en")
        print("[INFO] Whisper model loaded.")

    def start_av_stream(self, port: int = 1935, width: int = 640, height: int = 480) -> None:
        '''
        Initialize audio-visual streaming
        
        Args:
            port (int): RTSP port number
            width (int): Video width
            height (int): Video height
        '''
        print("[INFO] Enabling AV streaming service...")
        res_enable: Response = self.enable_av_streaming_service()
        if res_enable.status_code != 200:
            raise RuntimeError(f"Failed to enable AV! status={res_enable.status_code}")

        print(f"[INFO] Starting AV stream on port {port} ({width}x{height})...")
        res_start: Response = self.start_av_streaming(f"rtspd:{port}", width=width, height=height)
        if res_start.status_code != 200:
            raise RuntimeError(f"Start AV failed! status={res_start.status_code}")

        self.rtsp_url = f"rtsp://{self.ip}:{port}"
        print("[INFO] RTSP URL =", self.rtsp_url)
        

 

    def _read_audio_stream(self) -> None:
        '''Thread worker for reading audio data from RTSP stream'''
        try:
            container = av.open(self.rtsp_url, options={'rtsp_transport': 'tcp', 'stimeout': '5000000'})
            print("[AUDIO] RTSP audio opened.")
        except av.AVError as e:
            print(f"[AUDIO] Open failed: {e}")
            return

        audio_stream = next((s for s in container.streams if s.type == 'audio'), None)
        if not audio_stream:
            print("[AUDIO] No audio stream.")
            return

        try:
            for packet in container.demux(audio_stream):
                for frame in packet.decode():
                    frame_data: np.ndarray = frame.to_ndarray()
                    if frame_data.ndim > 1 and frame_data.shape[0] > 1:
                        frame_data = np.mean(frame_data, axis=0)
                    self.audio_queue.put((time.time(), frame_data.flatten()))
        except Exception as e:
            print(f"[AUDIO] Error: {e}")
        finally:
            print("[AUDIO] Thread exit.")

    def _process_audio(self) -> None:
        # '''Thread worker for processing audio data and detecting speech segments'''
        while not self._stop_event.is_set():
            try:
                timestamp: float
                frame_data: np.ndarray
                timestamp, frame_data = self.audio_queue.get(timeout=5)
                self.audio_buffer = np.concatenate((self.audio_buffer, frame_data))

                current_silent: bool = self.is_silent(frame_data, 44100, self.silence_threshold_db)
                
                if not current_silent:
                    if not self.voice_active:
                        print(">> Voice activity detected")
                        self.voice_active = True
                    self.last_voice_time = time.time()
                else:
                    if self.voice_active and self.last_voice_time:
                        silence_duration: float = time.time() - self.last_voice_time
                        if silence_duration >= self.silence_duration_threshold:
                            print(f">> Voice activity ended (silence duration {silence_duration:.1f}s)")
                            self._handle_utterance_end()
            except queue.Empty:
                if self.voice_active:
                    self._handle_utterance_end()
                continue

    def _handle_utterance_end(self) -> None:
        # '''Process completed speech segment and initiate transcription'''
        audio_duration: float = len(self.audio_buffer) / 44100
        if audio_duration >= self.min_utterance_length:
            self._transcribe_audio()
        else:
            print(f">> Discarding short utterance ({audio_duration:.1f}s < {self.min_utterance_length}s)")

        self.audio_buffer = np.array([], dtype=np.float32)
        self.voice_active = False
        self.last_voice_time = None

    def _transcribe_audio(self) -> None:
        try:
            if time.time() < self.ignore_transcript_until:
                print("[INFO] Transcript ignored during TTS cooldown.")
                return
            audio_resampled: np.ndarray = librosa.resample(
                self.audio_buffer, orig_sr=44100, target_sr=16000
            )
            result: Dict[str, Any] = self.model.transcribe(
                audio_resampled, language='en', fp16=False, task='transcribe'
            )
            transcript: str = result.get('text', "").strip()

            # Filter out common polite phrases
            banned_phrases: List[str] = ["thank you", "thanks", "thank"]
            if not any(bp in transcript.lower() for bp in banned_phrases):
                self.transcript_queue.put((time.time(), transcript))
                print(f"Transcription result: {transcript}")
        except Exception as e:
            print(f"Transcription failed: {e}")

    def _video_reader_thread(self) -> None:
        '''Thread worker for reading and processing video frames'''
        cap: cv2.VideoCapture = cv2.VideoCapture(self.rtsp_url, cv2.CAP_FFMPEG)
        if not cap.isOpened():
            print("[VIDEO] Failed to open video.")
            return

        try:
            while not self._stop_event.is_set():
                ret: bool
                frame: np.ndarray
                ret, frame = cap.read()
                if not ret:
                    break
                frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
                self.frame_queue.put(frame)
                time.sleep(0.01)
        except Exception as e:
            print(f"[VIDEO] Error: {e}")
        finally:
            cap.release()
            print("[VIDEO] Thread exit.")
            
    def chat_with_gpt(self, user_input: str) -> str:
        '''
        Sends user input to GPT and returns the response.
        '''
        with self._analysis_lock:  # Ensure thread safety with a lock
            try:
                # messages = [
                #     AIMessage(content=self.chat_ai_task),  # System message defining AI behavior
                #     HumanMessage(content=user_input)  # User input message
                # ]
                # 将用户输入添加到对话历史中
                self.conversation_history.append(HumanMessage(content=user_input))
                # 构造包含历史上下文的消息列表
                messages = [AIMessage(content=self.chat_ai_task)] + self.conversation_history

                # Call LangChain's ChatOpenAI model to generate a response
                response = self.llm.invoke(messages)
                self.conversation_history.append(AIMessage(content=response.content.strip()))


                return response.content.strip()  # Return the cleaned response

            except Exception as e:
                print(f"[GPT] Error calling invoke: {e}")  # Log any errors
                return "I'm sorry, I couldn't process your request."  # Return a fallback response


    def analyze_image_with_gpt(self, image_path: str) -> str:
        '''
        Sends an image to GPT using 'AIMessage' + 'HumanMessage(content=[...])' 
        with a 'type: image_url' object embedded in the content.
        
        This method relies on the backend's ability to process 'image_url' for 
        multimodal analysis. If the backend does not support image processing, 
        the request may not work as expected.
        
        Args:
            image_path (str): The file path of the image to be analyzed.

        Returns:
            str: GPT's response after analyzing the image.
        '''
        with self._analysis_lock:
            # Ensure thread safety by serializing access to this function
            try:
                # 1) Read the image and encode it in Base64 format
                with open(image_path, "rb") as f:
                    encoded_image = base64.b64encode(f.read()).decode('utf-8')
                # Define the instruction template for GPT's image analysis.
                    # When you want to change GPT's task, simply modify this template.
                    # For example, if you want GPT to analyze the emotions of the person in the image, 
                    # you can change this template accordingly.
                # template = '''
                #     Carefully analyze the given image. 
                #     Provide your best guess of what's in this image.
                # '''
                
                messages = [
                    AIMessage(content="You are an expert in analyzing image content."),
                    HumanMessage(content=[
                        {"type": "text", "text": self.vision_analysis_task},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encoded_image}"}}
                    ])
                ]

                # 3) Send the constructed messages to GPT using self.llm.invoke(...)
                response = self.llm.invoke(messages)

                # 4) Extract and return GPT's response
                return response.content.strip()

            except Exception as e:
                print(f"[GPT] Error calling invoke: {e}")  # Log the error
                return "GPT request failed."  # Return an error message if the request fails
            
    # def process_multimodal_task(self) -> None:
    #     cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)
    #     pdb.set_trace()
    #     while not self._stop_event.is_set():
    #         try:
    #             frame = self.frame_queue.get(timeout=0.1)
    #         except queue.Empty:
    #             continue 

    #         while not self.transcript_queue.empty():
    #             _, self.last_transcript = self.transcript_queue.get()

    #         if self.last_transcript:
    #             response = self.chat_with_gpt(self.last_transcript)
    #             self.speak(response)
    #             self.last_transcript = ""

    #         cv2.imshow("MistyVideo", frame)
    #         key = cv2.waitKey(1) & 0xFF
    #         if key == ord('q'):
    #             self._stop_event.set()
    #             break

    #     cv2.destroyAllWindows()

    # def process_multimodal_task(self) -> None:
    #     # '''
    #     # Handles video display and transcriptions in the main thread.
    #     # This function runs in the main thread to avoid OpenCV issues.
    #     # '''
    #     cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)
    #     cv2.resizeWindow("MistyVideo", 640, 480)

    #     while not self._stop_event.is_set():
    #         try:
                
    #             frame = self.frame_queue.get(timeout=0.1)
    #         except queue.Empty:
    #             continue 
    #         while not self.transcript_queue.empty():
    #             _, self.last_transcript = self.transcript_queue.get()

    #         if self.last_transcript:
    #             cv2.putText(
    #                 frame, self.last_transcript, (30, 50),
    #                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA
    #             )
    #         ##############################################
    #         # If you need to handle tasks based on the most recent speech input, process self.last_transcript here.
    #         # If you need to implement vision-based tasks, add frame analysis logic here.
    #         ##############################################
    #         cv2.imshow("MistyVideo", frame)
    #         key = cv2.waitKey(1) & 0xFF

    #         if key == ord('q'):  
    #             self._stop_event.set()
    #             break

    #     cv2.destroyAllWindows()
    
    def process_multimodal_task(self) -> None:
        '''
        Continuously processes video frames and speech input for multimodal interaction.
        
        - Displays the video feed with transcribed speech overlay.
        - Captures a photo and sends it to GPT for analysis when the user says "take a photo".
        - Ensures smooth handling of real-time inputs and outputs.
        '''
        
        # Create a resizable window to display the video feed
        cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)
        cv2.resizeWindow("MistyVideo", 640, 480)  # Set window size to 640x480

        while not self._stop_event.is_set():  # Run the loop until a stop signal is received
            try:
                frame = self.frame_queue.get(timeout=0.1)  # Retrieve the latest video frame
            except queue.Empty:
                continue  # Skip processing if no frame is available

            # Process the latest speech transcription from the queue
            while not self.transcript_queue.empty():
                _, self.last_transcript = self.transcript_queue.get()

            if self.last_transcript:  # If there is a new transcription
                # Overlay the spoken text onto the video feed
                cv2.putText(
                    frame, self.last_transcript, (30, 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA
                )

            # If the user says "take a photo", capture and analyze an image
            if "take a photo" in self.last_transcript.lower():
                # Generate a unique filename based on the current timestamp
                photo_filename = f"photo_{int(time.time())}.jpg"
                cv2.imwrite(photo_filename, frame)  # Save the current video frame as an image
                print(f"[PHOTO] {photo_filename} saved. Sending to GPT...")

                # Call analyze_image_with_gpt to analyze the captured image
                result = self.analyze_image_with_gpt(photo_filename)

                print("[GPT RESULT]", result)  # Print GPT's analysis result

                # Let the robot speak out GPT's analysis
                self.speak(result)

                # Clear the transcript after processing to prevent repeated triggers
                self.last_transcript = ""

            # Display the video feed with text overlay
            cv2.imshow("MistyVideo", frame)

            # Check for user input to exit (press 'q' to quit)
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                self._stop_event.set()  # Stop the loop when 'q' is pressed
                break

        # Clean up and close the video window when exiting
        cv2.destroyAllWindows()

   
    
    
    # def process_multimodal_task(self) -> None:
    #     cv2.namedWindow("MistyVideo", cv2.WINDOW_NORMAL)

    #     while not self._stop_event.is_set():
    #         try:
    #             frame = self.frame_queue.get(timeout=0.1)
    #         except queue.Empty:
    #             continue 

    #         while not self.transcript_queue.empty():
    #             _, self.last_transcript = self.transcript_queue.get()

    #         if self.last_transcript:
    #             response = self.chat_with_gpt(self.last_transcript)
    #             self.speak(response)
    #             self.last_transcript = ""

    #         cv2.imshow("MistyVideo", frame)
    #         key = cv2.waitKey(1) & 0xFF
    #         if key == ord('q'):
    #             self._stop_event.set()
    #             break

    #     cv2.destroyAllWindows()

  
    def _listen_for_ctrl_x(self) -> None: # this design for av stream
        # '''
        # Listens for Ctrl+X key press in a non-blocking way.
        # '''
        def on_press(key):
            if key in [keyboard.Key.ctrl_l, keyboard.Key.ctrl_r]:
                self._ctrl_pressed = True
            elif self._ctrl_pressed and hasattr(key, 'char') and key.char == 'x':
                print("[INFO] Detected Ctrl+X, shutting down...")
                self._stop_event.set()
                self.trigger_stop_event.set()  # 这里一定要用 trigger_stop_event

        def on_release(key):
            if key in [keyboard.Key.ctrl_l, keyboard.Key.ctrl_r]:
                self._ctrl_pressed = False

        listener = keyboard.Listener(on_press=on_press, on_release=on_release)
        listener.start()  

    def start_perception_loop(self) -> None:
        # 
        # Starts the main execution loop of the robot system while ensuring that 
        # OpenCV's GUI functions (cv2.imshow()) are handled within the main thread.
        # This method initializes various subsystems, starts required background 
        # threads for audio and video processing, and manages system shutdown.
        # 
        # 1. Start a listener for detecting the Ctrl+X key combination (non-blocking).
        #    This allows the user to gracefully terminate the system by pressing Ctrl+X.
        self._listen_for_ctrl_x()
        # 2. Stop any existing audio-visual (AV) streams to ensure a clean restart.
        #    This prevents conflicts if a previous session was running.
        print("[INFO] Stopping existing streams...")
        self.stop_av_streaming()
        # 3. Start a new AV stream for capturing video and audio.
        #    This is necessary for real-time video processing and speech recognition.
        print("[INFO] Starting AV stream...")
        self.start_av_stream()
        # 4. Load the Whisper model for speech-to-text transcription.
        #    Whisper is an AI model that converts spoken language into text.
        print("[INFO] Loading Whisper speech recognition model...")
        self.load_whisper_model()
        # 5. Launch background threads for handling different processing tasks.
        #    These threads run in parallel to the main thread and do not block UI updates.
        threads = [
            threading.Thread(target=self._read_audio_stream, daemon=True),  # Captures audio stream
            threading.Thread(target=self._process_audio, daemon=True),      # Processes audio and transcriptions
            threading.Thread(target=self._video_reader_thread, daemon=True) # Reads and processes video frames
        ]
  
        # Start all background threads
        for t in threads:
            t.start()
        print("[INFO] System is running. Press Ctrl+X to exit.")
        # 6. Run the main processing loop for video display and transcription handling.
        #    This ensures OpenCV functions like cv2.imshow() run within the main thread.
        self.process_multimodal_task()
        # 7. On exit, signal all background threads to terminate and perform cleanup.
        self._stop_event.set()  # Notify all threads to stop
        print("[MAIN] System shutdown.")


if __name__ == "__main__":
    misty = Robot('67.20.198.102')

    misty.start_perception_loop()


##### If the task is event trigered  task， here is an example of how to design：
from CUBS_Misty import Robot
from typing import Any, Dict
import cv2
import time
import base64
from langchain.schema.messages import HumanMessage, AIMessage
from langchain_community.chat_models import ChatOpenAI
import threading

def event_filter(name: str, comparison_operator: str, comparison_value: Any) -> Dict[str, Any]:
    '''
    Creates a dictionary for filtering event properties based on a condition.
    
    :param name: The name of the property to filter on.
    :param comparison_operator: A string representing the comparison operator (e.g. "=", ">", "<").
    :param comparison_value: The value against which the property is compared.
    :return: A dictionary containing the filter condition details.
    '''
    return {
        "Property": name,
        "Inequality": comparison_operator,
        "Value": comparison_value
    }

class CustomRobot(Robot):
    def __init__(self, ip: str, api_key: str):
        '''
        Initializes the CustomRobot with the given IP address and API key.
        
        :param ip: The IP address of the robot.
        :param api_key: The API key for GPT-related calls.
        '''
        super().__init__(ip)
        self.api_key = api_key
        
        # Instantiate the ChatOpenAI class with your chosen model, API key, and temperature settings.
        self.llm = ChatOpenAI(
            model="gpt-4o",
            openai_api_key=api_key,
            temperature=0.7,
        )
        
        # A predefined task or prompt for the GPT model to perform when analyzing images.
        self.vision_analysis_task = '''
            Analyze the photo carefully and describe the main elements and any notable details you observe.
            Provide insights into what is happening or what the image might represent.
        '''

    def capture_and_analyze_photo(self):
        '''
        Captures a photo from the video feed and analyzes it using GPT.
        '''
        try:
            # Attempt to retrieve the most recent frame from the frame queue with a small timeout.
            frame = self.frame_queue.get(timeout=0.1)
            
            # Create a filename for the photo using the current time to ensure uniqueness.
            photo_filename = f"photo_{int(time.time())}.jpg"
            
            # Save the retrieved frame as an image file.
            cv2.imwrite(photo_filename, frame)
            print(f"[PHOTO] {photo_filename} saved.")
            
            # Call the analyze_image_with_gpt method to describe the image content.
            result = self.analyze_image_with_gpt(photo_filename)
            
            # Print GPT’s result in the console for debugging or confirmation.
            print("[GPT RESULT]", result)
            
            # Have the robot speak out the result (assuming a speak method is available).
            self.speak(result)
        except Exception as e:
            # If the frame capture or image analysis fails, print out the error.
            print(f"Failed to capture or analyze photo: {e}")

    def analyze_image_with_gpt(self, image_path: str) -> str:
        '''
        Analyzes an image using GPT to describe its content.

        :param image_path: The path to the image file that needs to be analyzed.
        :return: A string containing GPT’s analysis of the image.
        '''
        try:
            # 1) Read the image file in binary mode and encode it into Base64 string.
            with open(image_path, "rb") as f:
                encoded_image = base64.b64encode(f.read()).decode('utf-8')

            # 2) Construct messages for GPT that include both a text instruction and the Base64-encoded image.
            messages = [
                # AIMessage acts as a system or "assistant" context to guide GPT’s behavior.
                AIMessage(content="You are an expert in analyzing image content."),
                
                # HumanMessage includes the prompt and the image data. 
                HumanMessage(content=[
                    {"type": "text", "text": self.vision_analysis_task},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encoded_image}"}}
                ])
            ]

            # 3) Invoke the GPT model (ChatOpenAI) with the constructed messages.
            response = self.llm.invoke(messages)

            # 4) Extract and return GPT's response by stripping extra whitespace.
            return response.content.strip()

        except Exception as e:
            # If there's an issue with invoking GPT, print an error message and return a default string.
            print(f"[GPT] Error calling invoke: {e}")
            return "GPT request failed."

    def register_head_touch(self) -> None:
        '''
        Registers the head touch event with a callback function that captures and analyzes a photo 
        when the robot's head is touched.
        '''
        
        # Define a callback function that will be triggered upon a head touch event.
        def head_touch_callback(data):
            '''
            The callback function triggered by a head touch event.
            
            :param data: Event data passed from the robot’s event system.
            '''
            print("[INFO] Head touched. Event successfully triggered.")
            print("[INFO] Capturing and analyzing photo...")
            
            # Upon detecting head touch, call the method to capture and analyze a photo.
            self.capture_and_analyze_photo()

        # Register the head touch event, specifying the event type, conditions, debounce time, 
        # keep_alive property, and the callback function.
        self.register_event(
            event_type="TouchSensor",
            event_name="HeadTouch",
            condition=[
                event_filter("sensorPosition", "=", "HeadFront")
            ],
            debounce=500,
            keep_alive=True,
            callback_function=head_touch_callback
        )

    def regist_and_run_event(self):
        '''
        Registers the head touch event and starts the robot's event loop.
        This method is often run in a separate thread to keep the main thread available.
        '''
        self.register_head_touch()
        self.start()  # Start the event handling loop in the parent class (Robot).

    def q(self) -> None:
        '''
        Starts the main perception loop of the robot. This includes:
        
        1) Listening for Ctrl+X to exit.
        2) Stopping any existing audio/video streaming.
        3) Starting new AV streams for video and audio.
        4) Loading the Whisper model for speech recognition.
        5) Spinning up background threads to handle audio, video, and event registration.
        6) Running the main multimodal processing loop.
        7) Handling system shutdown.
        '''
        # 1. Start a listener for detecting the Ctrl+X key combination in a non-blocking manner.
        self._listen_for_ctrl_x()

        # 2. Stop any existing AV streams to ensure a fresh start.
        print("[INFO] Stopping existing streams...")
        self.stop_av_streaming()

        # 3. Start a new AV stream for both video and audio.
        print("[INFO] Starting AV stream...")
        self.start_av_stream()

        # 4. Load the Whisper model for real-time or near-real-time speech transcription.
        print("[INFO] Loading Whisper speech recognition model...")
        self.load_whisper_model()

        # 5. Create and launch background threads for different tasks:
        #    - Reading the audio stream
        #    - Processing audio data
        #    - Reading video frames
        #    - Registering events and running the event loop
        threads = [
            threading.Thread(target=self._read_audio_stream, daemon=True),
            threading.Thread(target=self._process_audio, daemon=True),
            threading.Thread(target=self._video_reader_thread, daemon=True),
            threading.Thread(target=self.regist_and_run_event, daemon=True)  # Event registration and loop
        ]

        # Start each of the background threads.
        for t in threads:
            t.start()

        print("[INFO] System is running. Press Ctrl+X to exit.")

        # 6. Run the main loop that handles video display (OpenCV) and transcription events.
        self.process_multimodal_task()

        # 7. On exit, set a stop event flag for all threads and perform system cleanup.
        self._stop_event.set()
        print("[MAIN] System shutdown.")

def main():
    '''
    The main entry point of the script. Initializes and starts the CustomRobot's functionalities.
    '''
    # Robot IP and GPT API key (replace with your own valid credentials).
    misty_ip = '67.20.199.138'
    api_key = 'YOUR_OPENAI_API_KEY_HERE'  # 请设置您的OpenAI API Key
    
    # Create an instance of CustomRobot with the provided IP and API key.
    misty = CustomRobot(misty_ip, api_key)
    
    try:
        # Register the robot's head touch event.
        misty.register_head_touch()
        
        # Start the main perception loop, which includes AV streaming and event handling.
        misty.start_perception_loop()
    except Exception as e:
        # If there's an error during setup or the main loop, print it out.
        print(f"An error occurred: {e}")

# If this file is run directly, call the main function.
if __name__ == "__main__":
    main()

    
"""
import os
config_list = autogen.config_list_from_json(env_or_file=os.path.join(os.path.dirname(os.path.dirname(__file__)), "OAI_CONFIG_LIST.json"))
llm_config = {"config_list": config_list, "cache_seed": None}


# ----------------------------------------------------------------------
# 2. 草案阶段（Draft）内对话：助理 + 反思 + Teachability
# ----------------------------------------------------------------------

# 2.1 定义草案助理（使用 ConversableAgent）
#    （此处的 system_message 可替换为适合 Event 的提示）
misty_draft_perception_code_assistant = autogen.ConversableAgent(
    name="misty_draft_perception_code_assistant",
    llm_config=llm_config,
    system_message=SystemMessage  # 你可以自定义更符合“Event”上下文的system message
)

# 2.2 给草案助理添加 Teachability
misty_draft_perception_code_assistant_teachability = Teachability(
    verbosity=1,              # 0: 仅基础信息, 1: 含内存操作日志, 2: 含分析器消息, 3: 含所有memo列表
    reset_db=False,           # True 表示清空记忆数据库
    path_to_db_dir="./DB/misty_perception_db",
    recall_threshold=1.5      # 越大越容易召回更多但可能不相关的记忆
)
misty_draft_perception_code_assistant_teachability.add_to_agent(misty_draft_perception_code_assistant)

# 2.3 定义反思助理（如果需要类似 reflection 的逻辑）
#     你也可根据需要修改 system_message，如仅简单设置 "You are a reflection assistant

misty_perception_code_reflection_assistant = autogen.ConversableAgent(
    name="misty_perception_code_reflection_assistant",
    llm_config=llm_config,
    system_message=misty_perception_code_reflection_message
)

# 2.4 草案对话环节的 GroupChat
misty_draft_perception_code_groupchat = autogen.GroupChat(
    agents=[
        misty_draft_perception_code_assistant, 
        misty_perception_code_reflection_assistant
    ],
    messages=[],
    speaker_selection_method="round_robin",  # 轮流发言
    allow_repeat_speaker=False,
    max_round=5,
)

# 2.5 定义草案 GroupChat 的 Manager
#     这里 is_termination_msg 的触发条件可根据需要修改，如检测到 “EVENTAPPROVED”
misty_draft_perception_code_manager = autogen.GroupChatManager(
    name="misty_draft_perception_code_manager",
    groupchat=misty_draft_perception_code_groupchat,
    llm_config=llm_config,
    # 触发终止的条件，可自定义
    is_termination_msg=lambda x: x.get("content", "").find("PERCEPTIONAPPROVED") >= 0,
)

# 2.6 将草案阶段的 GroupChat Manager 封装成一个 SocietyOfMindAgent
draft_perception_code_response_preparer = (
    "Extract the final generated code from our conversation and "
    "respond with it exactly as it is, WITHOUT MAKING ANY MODIFICATIONS."
)
Draft_perception_Code = SocietyOfMindAgent(
    name="Draft_perception_Code",
    chat_manager=misty_draft_perception_code_manager,
    llm_config=llm_config,
    response_preparer=draft_perception_code_response_preparer
)


# ----------------------------------------------------------------------
# 3. 最终阶段：与用户代理（或代码代理）交互
# ----------------------------------------------------------------------

# 3.1 定义执行代码的代理
#     注意这里人机交互模式 human_input_mode="ALWAYS" 表示每次都提示人工输入
misty_perception_code_interpreter = autogen.UserProxyAgent(
    name="misty_perception_code_interpreter",
    human_input_mode="ALWAYS",
    code_execution_config={
        "work_dir": "code/mistyPy", 
        "use_docker": False,
    },
    default_auto_reply=""
)

# 3.2 定义最终对话的 GroupChat（与 Draft_Event_Code 和用户/代码代理 对话）
misty_perception_groupchat = autogen.GroupChat(
    agents=[Draft_perception_Code, misty_perception_code_interpreter],
    messages=[],
    speaker_selection_method="round_robin",
    allow_repeat_speaker=False,
    max_round=50
)

# 3.3 定义最终 GroupChat 的 Manager
misty_perception_manager = autogen.GroupChatManager(
    name="misty_perception_manager",
    groupchat=misty_perception_groupchat,
    llm_config=llm_config,
    #  "ALLSET"
    is_termination_msg=lambda x: x.get("content", "").find("ALLSET") >= 0,
)

# 3.4 将整个对话管理进一步封装成 SocietyOfMindAgent
PerceptionAgent_response_preparer = (
    "Extract the final generated code from our conversation and "
    "respond with it exactly as it is, WITHOUT MAKING ANY MODIFICATIONS."
)

# （可选）提取 JSON 的辅助函数
def extract_json(content):
    # 如果内容以 ```json 开头，先去掉它
    if content.startswith("```json"):
        content = content.lstrip("```json").strip()

    # 如果内容以 ``` 结尾，先去掉它
    if content.endswith("```"):
        content = content.rstrip("```").strip()

    try:
        return json.loads(content)
    except json.JSONDecodeError:
        print("Failed to parse JSON. Content might be incorrectly formatted.")
        return None



def my_hook(sender, message, recipient, silent):
    # pdb.set_trace()
    if recipient.name != 'misty_perception_manager': 
        return message
    
    # 收集历史消息
    history_list = []
    history_message_sources = []
    for manager, message_list in sender._oai_messages.items():
        for m in message_list:
            # m 应该是字典，包含 'content' 和 'name'，否则可能 KeyError
            history_list.append(m['content'])
            history_message_sources.append(m['name'])

    # 确保有足够的历史记录
    if len(history_list) <= 1:
        # 如果历史不足，则不做JSON解析，直接返回原消息
        return message

    # 尝试解析 JSON
    json_plan_task = extract_json(history_list[1])
    if not json_plan_task:
        # 解析失败就返回原消息
        return message
    
    # 从 JSON 中提取信息
    Misty_IP = json_plan_task.get('Misty_IP', 'UNKNOWN_IP')
    API_KEY = json_plan_task.get('API_KEY', 'UNKNOWN_API_KEY')
    task_for_this_agent = json_plan_task.get(sender.name, [])

    # 确保有足够的消息源信息
    if not history_message_sources:
        return message  # or 以你想要的方式处理
    
    last_source = history_message_sources[-1]

    # 根据最后一条消息的来源来分支
    if last_source == 'PlanAgent':
        # 简单模板
        new_message = (
            f"Misty_IP: {Misty_IP}\n\nAPI_KEY: {API_KEY}\n\n"
            "YOURTASK:\n" +
            "\n".join(f"{i+1}. {task}" for i, task in enumerate(task_for_this_agent))
        )
        # 别忘了 return
        return new_message
    else:
        # 复杂模板
        last_message = history_list[-1] if history_list else 'No previous messages'
        new_message = (
            f"Misty_IP: {Misty_IP}\n\nAPI_KEY: {API_KEY}\n\n"
            "YOUR TASK:\n" +
            "\n".join(f"{i+1}. {task}" for i, task in enumerate(task_for_this_agent)) +
            (
                f"\n\nPrevious Final Result: You must use this completed code as it is, "
                f"without making any modifications, to accomplish our task. "
                f"Previous Agent: {last_source if last_source else 'Unknown Agent'}"
            ) +
            (
                f"\n\nFinal code for {last_source if last_source else 'Unknown Agent'}:\n"
                f"{last_message}"
            )
        )
        return new_message


PerceptionAgent = SocietyOfMindAgent(
    name="PerceptionAgent",
    chat_manager=misty_perception_manager,
    llm_config=llm_config,
    response_preparer=PerceptionAgent_response_preparer,
)

# 如果需要对消息做 hook 处理，则注册 hook
PerceptionAgent.register_hook("process_message_before_send", my_hook)
